{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLOapP/oFC/9hRl+8Jgrhs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mxn170019/Deep-Learning-Exercises/blob/master/t5_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWjACycFK2Fb"
      },
      "source": [
        "# !pip install pytorch_lightning\n",
        "# !pip install nlp\n",
        "# !pip install transformers\n",
        "# !pip install wandb\n",
        "# !pip install rouge_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujS5RKLKZpy"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pytorch_lightning.loggers import WandbLogger, MLFlowLogger\n",
        "from nlp import load_metric\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF5oYaysKxeB"
      },
      "source": [
        "import wandb\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "YOUR_API_KEY = '3d73b64bb7cb5ce2689dcb1f16e7d9f22f0937ec'\n",
        "os.environ[\"WANDB_API_KEY\"] = YOUR_API_KEY\n",
        "wandb_logger = WandbLogger(project='mxn170019')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuHLg-y38dJq"
      },
      "source": [
        "!wandb login 3d73b64bb7cb5ce2689dcb1f16e7d9f22f0937ec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrJERwzX83dg"
      },
      "source": [
        "wandb.init(project=\"transformers_tutorials_summarization\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezc4TcUDPGYK"
      },
      "source": [
        "# !git clone https://github.com/mahnazkoupaee/WikiHow-Dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO_Uorf_8Am7"
      },
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = '3d73b64bb7cb5ce2689dcb1f16e7d9f22f0937ec'\n",
        "wandb_logger = WandbLogger(project='wikohow-t5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7gYifNnLyBc"
      },
      "source": [
        "from nlp import list_datasets\n",
        "datasets_list = list_datasets()\n",
        "print(', '.join(dataset.id for dataset in datasets_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smJmvTEHNPJ0"
      },
      "source": [
        "# !curl https://public.boxcloud.com/d/1/b1!stNjKk1-utRCbLV382UJxByQCEe_ytM5Cpylk3P8C_ZKnCNW_-fkrHwk5y16u3rOGKC4bHyxPCJcfku32sjfZD5bbRUZUEC8NTbueBRBOzgUOFx8WskI31eFfTOIhl_PqiEFrqXSmgOb4otKZQEaBNZ6Mo5oMIWjq6v0l3dIfbAmHCyJRMzoEGHIaMOx61-eoTYDryjjopJVd-Jf94sbh7pwaMAt65DDQwAa-mMnQkHc59FOm8LCPkrmMfDtv3BAHBMm7VYQIMWuofgVPUfWLcsNTqbxCXeCE-MJ9auCCzePM8GKsKQ1-DF0ehMe3JFoawGa9YGvilEbUBsAe2NZQrxP3rdaf6-1JpGXPaT6GMYsYQm0sVvJ-zrY-lOUDifm7kg21oGMsMJsIhpvz42hB5wkLs55xo4MQC5cOtCEko4-UmHF_2s7wn1UtY9cX-4bwBF801gRHnGnH9SDJhAAkTtH15urOUkjaugnTYxXy3I2BteYOGdcdtXNuiFtIXLCAPchZ7QnUV09U0TNDnoY8H6f_AtmqVqz6Tk75HfdCcT5ScKsf8KfnDqY5NGZbwRGZqzD0W-JIqTqVJpznCWIZN4SfLG2F9JSG1pF-YKTEIref54LrPG9DCfkVrPdIho8ZJfAMUTkiGqii35W_hT1cNjCPuGFvsjzjeXxnfk-rCIHUJp5yLe5ZTyLPEnnDNei9fK8sz1gfNJGTaDxTKIlni-DzoJDkdLtBLTa2QC8UO-xLzOMErowXlFeA8aiJNCor5Q0GcNckXpwmOA-UwWtkr8U59Nvm-8Uh1Lhool1-U8X8uchto_jS3kJowkCWnVkn9dIz020PYX276kaMlA2hoQzQEnspmu8ZSpcpHtgIXBXoGChoDEVT6cTsKjI8zTKkvtMmC5mB-bExxGNSJXA0B9Joyms8xfpcMO613CqeqLeSQ_zcht__n5zejfP5nwvVLP5AAy3pvWtD-zctZ-otsyRm6rXBYFac223EhEAQiEx4kLCxyVH3BD6GQNnECZ-45yTf2Xs6r5TrzKjansqqGmJ5In5Sou2G4KqiK5L86wkzra2yYwJ0YP75uuDJVAvxRnY6dRmIw_z5BeuDK_z0ArBsOXPvAMKmCZ9tbLnk8TAyJYdy0dDRjsEmHTAKq8vaXG0zfuyDALVAkWtdQcannZbemSRZXQMDYNmaKHGU9vsSyJt23jFuMJm4R25mLu-UhA0asXfGP01X-vqJkAygIzSKiV8XrvPsZJWzTfKYNpWmHS_wUbgf2VsPA../download >wikihowAll.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuRL2idb7fnY"
      },
      "source": [
        "# !curl https://public.boxcloud.com/d/1/b1!zwgDDRPrcByxW5yjZs1KgwdQutTv-DglV1feAnDVGUYw3hKV8XfLCVu63EBtGAW5odQq-vHUYxO0u9BbPjF3RIDthESACFxOqn_PkJK2PIlzDt5WVK6q25-A_zfm3-DY6niMNECtJPZdluK780XlSLlTUyp3CYz40oNystcBgQZI54ToaYk1E3hb1BB39I8A1kkobk9-YH1-5JjOzC66xMLKSMGQnS4cNTccs20620aDZPC4IQeSrnd5cKSLlk854VueCmqYawbklRAeFKatXiIgDzHnYnEsPyA6kvHee6uI7xgny7v4-Ry7EhYDD7OjVeuvimOdHedW4FneQTxmcuDf-wXAju-8YPvmAaRUqMLpzLNpugtVqfwEqMgj2RY-NwZXsegFCDsDwMmTrJSlycKq9pnoQDtcJ8nsv6w_Kc1GgmafDG0LZuRJHlrwrlnfJBwcXmTuPjxL7eDy8Dv_v_MtTy1-U2fPIJkOKhSHS1FP-25FJL7r3e4bY9c8kYA2iL1DUKjGplw3DAMLE7pzIXpIRRAHBOl7Ff3l9ZsT94iJ_ajGy5NjNTfKQ4jAH-O_l6Kl5bptmaN0O9how5wSsO8VfOlUS4uRzCCvyVNEqu_ch7vKpD56O_QHne-1FeH8MFFzDMHnB14xvi4kiND6SsgeRq0FPcuKjel4KSniZPhPXWeVdzj4Gk43IcvpdSL_UwarQkPCmYuPsw214dohDj6Wnyg9UTuOMR7hs8l5fFrBixZAesj6HZJt1DUA0KsP5zE9bCMyxZOqoUFsqD0msNFxpwtidMA6gkr-OpkCqr6vHaZbBuzuuqTL-Ribolo2IWMlT2JRLjncJNQuIT_zo2KgakdL3DqoII5NjtIT5HMgUv4rY0UifgsxxzvkAp5Z1TQ6JyEv5khHv_kAlHmDPoj-HsTvmp8pDm44mff_ftZnap77mayqKxklyxGWQ_4V7yB1Y7MMt9e75zYpJ10Wc0K5XLeYtwO5Vyee0dsY_aVzzx5d-R_oEpLeRQp73U2EEwNI1W_rGjzHBYCRiJW8bB3v9zuOyV5X0gVToScLR081_4HT85abUwiJWhcncGeJOUeJLRv6HNJU8z0K_0TGkX6YBPYqtpJOQwVdc6Kl2Tub_i80kzQxMJP87ZmptMoPiBZgMUMYsDXSsm0fn02SySJn0MpRoEjC8Kc3TkDIQ8WiCwzxzP_dTZsgT9lk0vEAnY_F9hKr06K1W0wUcdENSNCmSNQI7fyk8550GFWK_BTM1cacpK6TGgPO45c./download>wikihowSep.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrO0v4DmiCAq"
      },
      "source": [
        "# 1) https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save the file under <path/to/folder>/wikihowAll.csv\n",
        "# 2) https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag and save the file under <path/to/folder>/wikihowSep.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7iDwedNCV7"
      },
      "source": [
        "from nlp import load_dataset\n",
        "dataset = load_dataset('wikihow', 'all', data_dir='/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKbZl5h48yx1"
      },
      "source": [
        "print(dataset.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0FUFGhH8yXp"
      },
      "source": [
        "print(\"Size of train dataset: \", dataset['train'].shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rT5gRnDQWDSp"
      },
      "source": [
        "df = dataset['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvAcmW2MWGhI"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8jZqWIs8wlh"
      },
      "source": [
        "print(\"Size of Validation dataset: \", dataset['validation'].shape)\n",
        "print(\"Size of ca test dataset: \", dataset['test'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3AsIKyQ8KGp"
      },
      "source": [
        "print(dataset['train'][0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNj3yyGAmXK"
      },
      "source": [
        "print(\" Example of text: \", dataset['train'][2]['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbLUSN7Aoc7"
      },
      "source": [
        "print(\" Example of Summary: \", dataset['train'][0]['headline'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqslOXcDAtbb"
      },
      "source": [
        "print(\" Example of Title: \", dataset['train'][0]['title'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Z2aUAo2GwG"
      },
      "source": [
        "# Estimate average length of Text and Summary\n",
        "tiny_dataset = dataset['train'].select(list(range(0, 100)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTKLEERWXcJ_"
      },
      "source": [
        "tiny_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sQ2tnrpWPCM"
      },
      "source": [
        "len(tiny_dataset['text']),type(tiny_dataset['text']),len(tiny_dataset['headline']),type(tiny_dataset['headline']),"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cARU_1lv2Qzi"
      },
      "source": [
        "text_len = []\n",
        "summary_len=[]\n",
        "for i in range(len(tiny_dataset)):\n",
        "    example = tiny_dataset[i]\n",
        "    text_example = example['text']\n",
        "    text_example = text_example.replace('\\n','')\n",
        "    text_words = text_example.split()\n",
        "    text_len.append(len(text_words))\n",
        "    summary_example = example['headline']\n",
        "    summary_example = summary_example.replace('\\n','')\n",
        "    summary_words = summary_example.split()\n",
        "    summary_len.append(len(summary_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VONMkQyYkR_"
      },
      "source": [
        "# pd.read_csv('/content/wikihowSep.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW9nFNJn2YbH"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(text_len)\n",
        "plt.title('Text Length Distribution - First 100 examples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "runWxYZ52b__"
      },
      "source": [
        "plt.hist(summary_len)\n",
        "plt.title('Summary Length Distribution - First 100 examples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj00JGK82e7O"
      },
      "source": [
        "print(\"Average Length of text: \", sum(text_len)/len(text_len))\n",
        "print(\"Average Length of Summary: \", sum(summary_len)/len(summary_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xsBMl1r2yfG"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoQHi7dw2k8M"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gq4KO5CL2qMe"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "  def __init__(self, hparams):\n",
        "    super(T5FineTuner, self).__init__()\n",
        "    self.hparams = hparams        \n",
        "    self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "    self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
        "    self.rouge_metric = load_metric('rouge') \n",
        "\n",
        "    if self.hparams.freeze_embeds:\n",
        "      self.freeze_embeds()\n",
        "    if self.hparams.freeze_encoder:\n",
        "      self.freeze_params(self.model.get_encoder())\n",
        "      assert_all_frozen(self.model.get_encoder())\n",
        "        \n",
        "            \n",
        "    n_observations_per_split = {\"train\": self.hparams.n_train,\"validation\": self.hparams.n_val,\"test\": self.hparams.n_test,}\n",
        "    self.n_obs = {k: v if v >= 0 else None for k, v in n_observations_per_split.items()}\n",
        "        \n",
        "    \n",
        "  def freeze_params(self, model):\n",
        "    for par in model.parameters():\n",
        "      par.requires_grad = False\n",
        "            \n",
        "            \n",
        "  def freeze_embeds(self):\n",
        "      \"\"\"Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\"\"\"\n",
        "      try:\n",
        "        self.freeze_params(self.model.model.shared)\n",
        "        for d in [self.model.model.encoder, self.model.model.decoder]:\n",
        "            freeze_params(d.embed_positions)\n",
        "            freeze_params(d.embed_tokens)\n",
        "      except AttributeError:\n",
        "        self.freeze_params(self.model.shared)\n",
        "        for d in [self.model.encoder, self.model.decoder]:\n",
        "            self.freeze_params(d.embed_tokens)\n",
        "    \n",
        "  def lmap(self, f, x):\n",
        "      \"\"\"list(map(f, x))\"\"\"\n",
        "      return list(map(f, x))\n",
        "    \n",
        "\n",
        "  def is_logger(self):\n",
        "      return self.trainer.global_rank <= 0\n",
        "    \n",
        "    \n",
        "  def parse_score(self, result):\n",
        "      return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}\n",
        "        \n",
        "  def forward(\n",
        "    self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
        "):\n",
        "      return self.model(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          decoder_input_ids=decoder_input_ids,\n",
        "          decoder_attention_mask=decoder_attention_mask,\n",
        "          lm_labels=lm_labels,\n",
        "  )\n",
        "\n",
        "  def _step(self, batch):\n",
        "      lm_labels = batch[\"target_ids\"]\n",
        "      lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "      outputs = self(\n",
        "          input_ids=batch[\"source_ids\"],\n",
        "          attention_mask=batch[\"source_mask\"],\n",
        "          lm_labels=lm_labels,\n",
        "          decoder_attention_mask=batch['target_mask']\n",
        "      )\n",
        "\n",
        "      loss = outputs[0]\n",
        "\n",
        "      return loss\n",
        "    \n",
        "    \n",
        "  def ids_to_clean_text(self, generated_ids):\n",
        "      gen_text = self.tokenizer.batch_decode(\n",
        "          generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "      )\n",
        "      return self.lmap(str.strip, gen_text)\n",
        "    \n",
        "    \n",
        "  def _generative_step(self, batch) :\n",
        "      \n",
        "      t0 = time.time()\n",
        "      \n",
        "      generated_ids = self.model.generate(\n",
        "          batch[\"source_ids\"],\n",
        "          attention_mask=batch[\"source_mask\"],\n",
        "          use_cache=True,\n",
        "          decoder_attention_mask=batch['target_mask'],\n",
        "          max_length=150, \n",
        "          num_beams=2,\n",
        "          repetition_penalty=2.5, \n",
        "          length_penalty=1.0, \n",
        "          early_stopping=True\n",
        "      )\n",
        "      preds = self.ids_to_clean_text(generated_ids)\n",
        "      target = self.ids_to_clean_text(batch[\"target_ids\"])\n",
        "          \n",
        "      gen_time = (time.time() - t0) / batch[\"source_ids\"].shape[0]  \n",
        "  \n",
        "      loss = self._step(batch)\n",
        "      base_metrics = {'val_loss': loss}\n",
        "#         rouge: Dict = self.calc_generative_metrics(preds, target)\n",
        "      summ_len = np.mean(self.lmap(len, generated_ids))\n",
        "      base_metrics.update(gen_time=gen_time, gen_len=summ_len, preds=preds, target=target)\n",
        "      self.rouge_metric.add_batch(preds, target)\n",
        "      \n",
        "#         rouge_results = self.rouge_metric.compute() \n",
        "#         rouge_dict = self.parse_score(rouge_results)\n",
        "#         base_metrics.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "      \n",
        "      return base_metrics\n",
        "    \n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "      loss = self._step(batch)\n",
        "\n",
        "      tensorboard_logs = {\"train_loss\": loss}\n",
        "      return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "  \n",
        "  def training_epoch_end(self, outputs):\n",
        "      avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "      tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
        "      return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "      return self._generative_step(batch)\n",
        "    \n",
        "  \n",
        "  def validation_epoch_end(self, outputs):\n",
        "      \n",
        "      avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "      tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "      \n",
        "      rouge_results = self.rouge_metric.compute() \n",
        "      rouge_dict = self.parse_score(rouge_results)\n",
        "  \n",
        "      tensorboard_logs.update(rouge1=rouge_dict['rouge1'], rougeL=rouge_dict['rougeL'])\n",
        "      \n",
        "      ## Clear out the lists for next epoch\n",
        "      self.target_gen= []\n",
        "      self.prediction_gen=[]\n",
        "      return {\"avg_val_loss\": avg_loss, \n",
        "              \"rouge1\" : rouge_results['rouge1'],\n",
        "              \"rougeL\" : rouge_results['rougeL'],\n",
        "              \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "      model = self.model\n",
        "      no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "      optimizer_grouped_parameters = [\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": self.hparams.weight_decay,\n",
        "          },\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": 0.0,\n",
        "          },\n",
        "      ]\n",
        "      optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "      self.opt = optimizer\n",
        "      return [optimizer]\n",
        "  \n",
        "  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None, using_native_amp=False):\n",
        "      if self.trainer.use_tpu:\n",
        "          xm.optimizer_step(optimizer)\n",
        "      else:\n",
        "          optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      self.lr_scheduler.step()\n",
        "\n",
        "  def get_tqdm_dict(self):\n",
        "      tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
        "\n",
        "      return tqdm_dict\n",
        "    \n",
        "\n",
        "  def train_dataloader(self):   \n",
        "      n_samples = self.n_obs['train']\n",
        "      train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", num_samples=n_samples, args=self.hparams)\n",
        "      dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True, num_workers=4)\n",
        "      t_total = (\n",
        "          (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "          // self.hparams.gradient_accumulation_steps\n",
        "          * float(self.hparams.num_train_epochs)\n",
        "      )\n",
        "      scheduler = get_linear_schedule_with_warmup(\n",
        "          self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "      )\n",
        "      self.lr_scheduler = scheduler\n",
        "      return dataloader\n",
        "\n",
        "  def val_dataloader(self):\n",
        "      n_samples = self.n_obs['validation']\n",
        "      validation_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"validation\", num_samples=n_samples, args=self.hparams)\n",
        "      \n",
        "      return DataLoader(validation_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
        "  \n",
        "    \n",
        "  def test_dataloader(self):\n",
        "      n_samples = self.n_obs['test']\n",
        "      test_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"test\", num_samples=n_samples, args=self.hparams)\n",
        "        \n",
        "      return DataLoader(test_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyIxlUgQoy_1"
      },
      "source": [
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(pl.Callback):\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        logger.info(\"***** Validation results *****\")\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "            # Log results\n",
        "            for key in sorted(metrics):\n",
        "                if key not in [\"log\", \"progress_bar\"]:\n",
        "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "    def on_test_end(self, trainer, pl_module):\n",
        "        logger.info(\"***** Test results *****\")\n",
        "\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "\n",
        "            # Log and save results to file\n",
        "            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
        "            with open(output_test_results_file, \"w\") as writer:\n",
        "                for key in sorted(metrics):\n",
        "                    if key not in [\"log\", \"progress_bar\"]:\n",
        "                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcOVsDF05S92"
      },
      "source": [
        "#Define a DataSet class for the loader¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzB-7zK_5Srd"
      },
      "source": [
        "class wikihow(Dataset):\n",
        "    def __init__(self, tokenizer, type_path, num_samples, input_length, output_length, print_text=False):         \n",
        "      self.dataset =  load_dataset('wikihow', 'all', data_dir='data/', split=type_path)\n",
        "      if num_samples:\n",
        "            self.dataset = self.dataset.select(list(range(0, num_samples)))\n",
        "            self.input_length = input_length\n",
        "            self.tokenizer = tokenizer\n",
        "            self.output_length = output_length\n",
        "            self.print_text = print_text\n",
        "      \n",
        "    def __len__(self):\n",
        "        return self.dataset.shape[0]\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        text = text.replace('Example of text:', '')\n",
        "        text = text.replace('Example of Summary:', '')\n",
        "        text = text.replace('\\n','')\n",
        "        text = text.replace('``', '')\n",
        "        text = text.replace('\"', '')\n",
        "        \n",
        "        return text\n",
        "    \n",
        "    \n",
        "    def convert_to_features(self, example_batch):\n",
        "        # Tokenize contexts and questions (as pairs of inputs)\n",
        "        \n",
        "        if self.print_text:\n",
        "            print(\"Input Text: \", self.clean_text(example_batch['text']))\n",
        "#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n",
        "#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n",
        "        \n",
        "        input_ = self.clean_text(example_batch['text'])\n",
        "        target_ = self.clean_text(example_batch['headline'])\n",
        "        \n",
        "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n",
        "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "        \n",
        "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n",
        "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "       \n",
        "        return source, targets\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source, targets = self.convert_to_features(self.dataset[index])\n",
        "        \n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        target_ids = targets[\"input_ids\"].squeeze()\n",
        "\n",
        "        src_mask    = source[\"attention_mask\"].squeeze()\n",
        "        target_mask = targets[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQRprHU153Be"
      },
      "source": [
        "#Test the dataset function¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr7K6CSM5zAf"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "dataset = wikihow(tokenizer, 'validation', None, 512, 150, True)\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7feFefU5da_g"
      },
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgq_vZS86FSv"
      },
      "source": [
        "# data = dataset[50]\n",
        "# print()\n",
        "# print(\"Shape of Tokenized Text: \", data['source_ids'].shape)\n",
        "# print()\n",
        "# print(\"Sanity check - Decode Text: \", tokenizer.decode(data['source_ids']))\n",
        "# print(\"====================================\")\n",
        "# print(\"Sanity check - Decode Summary: \", tokenizer.decode(data['target_ids']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9BHWGxW67Er"
      },
      "source": [
        "100Define Arguments11dsfsdfxx111"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GblOjeMS6taC"
      },
      "source": [
        "args_dict = dict(\n",
        "    output_dir='/content/', # path to save the checkpoints\n",
        "    model_name_or_path='t5-small',\n",
        "    tokenizer_name_or_path='t5-small',\n",
        "    max_input_length=512,\n",
        "    max_output_length=150,\n",
        "    freeze_encoder=False,\n",
        "    freeze_embeds=False,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    train_batch_size=4,\n",
        "    eval_batch_size=4,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    n_gpu=1,\n",
        "    resume_from_checkpoint=None, \n",
        "    val_check_interval = 0.05, \n",
        "    n_val=1000,\n",
        "    n_train=-1,\n",
        "    n_test=-1,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
        "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
        "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
        "    seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4xGHTp17VS_"
      },
      "source": [
        "args = argparse.Namespace(**args_dict)\n",
        "print(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kAvrD3T7qJn"
      },
      "source": [
        "## Define Checkpoint function\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=3\n",
        ")\n",
        "\n",
        "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    # early_stop_callback=False,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        "    checkpoint_callback=checkpoint_callback,\n",
        "    val_check_interval=args.val_check_interval,\n",
        "    logger=wandb_logger,\n",
        "    callbacks=[LoggingCallback()],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At5DEzA875UZ"
      },
      "source": [
        "def get_dataset(tokenizer, type_path, num_samples, args):\n",
        "      return wikihow(tokenizer=tokenizer, type_path=type_path, num_samples=10,  input_length=args.max_input_length,output_length=args.max_output_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq03olF2ggr0"
      },
      "source": [
        "# tokenizer.add_special_tokens()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBmrjUYU-yic"
      },
      "source": [
        " pip install rouge_score rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkXd6OXV-jo-"
      },
      "source": [
        "model = T5FineTuner(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9se-HWKM-oa6"
      },
      "source": [
        "trainer = pl.Trainer(**train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HjmNLblnTI6"
      },
      "source": [
        "trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEpGow81nvl8"
      },
      "source": [
        "import textwrap\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSsHlpV9U3kR"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "dataset = wikihow(tokenizer, 'test', None, 512, 150, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhzzIPrZU7PH"
      },
      "source": [
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "it = iter(loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th88XUu3spOx"
      },
      "source": [
        "next(it)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBEcr1BeU9-2"
      },
      "source": [
        "batch = next(it)\n",
        "batch[\"source_ids\"].shape\n",
        "model.to('cuda')\n",
        "outs = model.generate(\n",
        "            batch[\"source_ids\"].cuda(),\n",
        "            attention_mask=batch[\"source_mask\"].cuda(),\n",
        "            use_cache=True,\n",
        "            decoder_attention_mask=batch['target_mask'].cuda(),\n",
        "            max_length=150, \n",
        "            num_beams=2,\n",
        "            repetition_penalty=2.5, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "dec = [tokenizer.decode(ids) for ids in outs]\n",
        "\n",
        "texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
        "targets = [tokenizer.decode(ids) for ids in batch['target_ids']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NIwVi4XVAVe"
      },
      "source": [
        "for i in range(32):\n",
        "    lines = textwrap.wrap(\"WikiHow Text:\\n%s\\n\" % texts[i], width=100)\n",
        "    print(\"\\n\".join(lines))\n",
        "    print(\"\\nActual Summary: %s\" % targets[i])\n",
        "    print(\"\\nPredicted Summary: %s\" % dec[i])\n",
        "    print(\"=====================================================================\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaO8-CxjVhYe"
      },
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xFXi4O1YrKm"
      },
      "source": [
        "# model.save_pretrained(\"/content/t5-wikihow/\")\n",
        "# tokenizer.save_pretrained(\"path/to/awesome-name-you-picked\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUGEbxYNVpxO"
      },
      "source": [
        "\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "model1 = AutoModelWithLMHead.from_pretrained(\"/content/t5-wikihow\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0_1FgRCXWiF"
      },
      "source": [
        "\n",
        "## Move to CUDA\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model1.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jLc6pHod2TH"
      },
      "source": [
        "\n",
        "text = \"\"\"\"\n",
        "Lack of fluids can lead to dry mouth, which is a leading cause of bad breath. Water\n",
        "can also dilute any chemicals in your mouth or gut that are causing bad breath., Studies show that\n",
        "eating 6 ounces of yogurt a day reduces the level of odor-causing compounds in the mouth. In\n",
        "particular, look for yogurt containing the active bacteria Streptococcus thermophilus or\n",
        "Lactobacillus bulgaricus., The abrasive nature of fibrous fruits and vegetables helps to clean\n",
        "teeth, while the vitamins, antioxidants, and acids they contain improve dental health.Foods that can\n",
        "be particularly helpful include:Apples — Apples contain vitamin C, which is necessary for health\n",
        "gums, as well as malic acid, which helps to whiten teeth.Carrots — Carrots are rich in vitamin A,\n",
        "which strengthens tooth enamel.Celery — Chewing celery produces a lot of saliva, which helps to\n",
        "neutralize bacteria that cause bad breath.Pineapples — Pineapples contain bromelain, an enzyme that\n",
        "cleans the mouth., These teas have been shown to kill the bacteria that cause bad breath and\n",
        "plaque., An upset stomach can lead to burping, which contributes to bad breath. Don’t eat foods that\n",
        "upset your stomach, or if you do, use antacids. If you are lactose intolerant, try lactase tablets.,\n",
        "They can all cause bad breath. If you do eat them, bring sugar-free gum or a toothbrush and\n",
        "toothpaste to freshen your mouth afterwards., Diets low in carbohydrates lead to ketosis — a state\n",
        "in which the body burns primarily fat instead of carbohydrates for energy. This may be good for your\n",
        "waistline, but it also produces chemicals called ketones, which contribute to bad breath.To stop the\n",
        "problem, you must change your diet. Or, you can combat the smell in one of these ways:Drink lots of\n",
        "water to dilute the ketones.Chew sugarless gum or suck on sugarless mints.Chew mint leaves.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_l2Gcjjd6vO"
      },
      "source": [
        "\n",
        "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "tokenized_text = tokenizer.encode(preprocess_text, return_tensors=\"pt\").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqDO-DBWd9Mc"
      },
      "source": [
        "summary_ids = model.generate(\n",
        "            tokenized_text,\n",
        "            max_length=150, \n",
        "            num_beams=2,\n",
        "            repetition_penalty=2.5, \n",
        "            length_penalty=1.0, \n",
        "            early_stopping=True\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fObJjIf3d_4U"
      },
      "source": [
        "\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1haL4JeBeCiz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}